%%
%% Larson Hogstrom 
%% MIT 18.335
%% Final Project
%% 5/12/2015

\documentclass[final,leqno,onefignum,onetabnum]{siamltex1213}

\usepackage{graphicx}
\usepackage{algorithm2e}
\graphicspath{ {"/Users/hogstrom/Dropbox (Personal)/cources_spring_2015_MIT/18_335_Numeric_Methods/NMF/figures/"} }
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\title{Numeric methods for Nonnegative matrix factorization \thanks{A final project for 18.335, Fall 2015}} 

\author{Larson Hogstrom\thanks{
(\email{hogstrom@mit.edu}). }}


\begin{document}

\maketitle
\slugger{mms}{xxxx}{xx}{x}{x--x}%slugger should be set to mms, siap, sicomp, sicon, sidma, sima, simax, sinum, siopt, sisc, or sirev

\begin{abstract}
This report introduces the framework for parts-based representations using NMF and focuses on the algorithms and numerical aspects of computation. 

\end{abstract}

\begin{keywords} dimensionality reduction, matrix factorization, least squares, nonnegativity, data representation \end{keywords}



\pagestyle{myheadings}
\thispagestyle{plain}
\markboth{TEX PRODUCTION}{USING SIAM'S \LaTeX\ MACROS}

\section{Introduction}

Nonnegative matrix factorization (NMF) is a dimensionality reduction technique that can be applied in a variety of domains. Like principal component analysis (PCA), NMF seeks to describe a large matrix of data using a small set of information dense components. Unlike PCA, which imposes orthogonality constraints on components, NMF requires nonnegativity after factorization. This procedure allows for only additive combinations of positive components to estimate the input matrix. The endpoint is a parts-based representation of the starting data where each component is more easily interpreted than PCA. 

A number of algorithms have been proposed in order to construct the NMF factorization with positivity constraints. Iterative algorithms, for example, build the WH factorization while converging on a local maximum of a specified object function. This project will examine two of such objective functions 1) the conventional least squares difference between the input and the factorization and 2) an objective function based on the Kullback-Leibler divergence2. The flop counts of algorithmic update rules for both approaches will both be assessed analytically and the different conditions for monotonic convergence will be examined. Both algorithms will be implemented in python using the ?numpy? and ?scipy? packages. The two approaches will be compared as follows: 1) In numeric simulations of input matrices with known contributions of signal and noise 2) using real-world gene expression data of cancer cells that have been exposed to different drugs grouped into known pharmacological classes. In simulations, the two algorithms will be compared by their accuracy after a fixed number of iterations. The approaches will also be compared in their ability to classify the gene expression profiles into known pharmacological groups.

This project compares multiple algorithms for computing NMF factorization including gradient and non gradient methods. The motivation 

The accuracy and convergence rates
affect of initialization conditions 

\section{Problem Definition and Multiplicative Update Rules}

Text leading to definition... \\

\begin{definition} For a nonnegative matrix $\textbf{A} \in \Re^{m x n} $, select a low-rank approximation of size k such that there are two nonnegative matrices \textbf{W} $ \in \Re^{m x k}$ and \textbf{H} $ \in \Re^{k x n}$ which minimizes a function such as 

\begin{equation}\label{ObjEuc} f( \textbf{W,H}) = \frac{1}{2} || \textbf{A - WH} || ^{2}_{F}\end{equation}
\end{definition}

KKT optimality conditions a stationary point for 




Other commonly used objective functions include Euclidian distance and Kullback-Leibler (KL) divergence. KL can be extended to a more general information-based framework using Renyi's divergence. (Devarajan, 2005). Here, a single parameter $\alpha$ is used to represent a continuum of distance measures and KL airises as a special case as $\alpha \to 1$. 

$$ KL(V || WH) = \sum_{ij}{[V_{ij} \log{ \frac{V_{ij}}{(WH)_{ij}} - V_{ij} + (WH)_{ij}} ]} $$

Regardless of the objective used to obtain them, the resulting W and H matrices are often interpreted directly. The matrix \textbf{W} $ \in \Re^{m x k}$ typically represents the k different components of signal in the original matrix. The \textbf{H} $ \in \Re^{k x n}$ matrix can be interpreted as weighting factors defining the linear combination of k components to reconstruct a sample in the original data matrix. In the application of NMF to gene expression data for example, each of the k columns of W represent a 'metagene' response. A small number of these metagene columns can be combined to estimate an original experimental column found in the original data matrix. The H matrix defines the weighting of each metagene. 


\section{Multiplicative Update Rule}

Text \\

\begin{algorithm}[H]
 \KwData{Input data matrix:  $\bf{A} \in \Re^{m x n}$}
 \KwResult{nonnegative factorization of \textbf{A} using k components, creating matrices \textbf{W} $ \in \Re^{m x k}$ and \textbf{H} $ \in \Re^{k x n}$ }

 initialization\;
$\bf{W} \gets $ random dense (m x k) matrix\\
$\bf{H} \gets $ random dense (k x n) matrix\\
 \For{i = 1 to \text{maxiter} }{
 	$\bf{H = H .* (W^T A) ./ (W^T WH)} $
	
 	$\bf{W = W .* (A H^T) ./ (WHH^T)} $	}
 \caption{Multiplicative update}
\end{algorithm} 

\section{Alternating Least Squares}

The alternating least squares can be viewed as a special case of block coordinate descent method. For this class of algorithms, the objective function is minimized with respect to coordinate vectors $x^{k}_{i}$. Multidimensional vectors are minimized in cyclical order, with one block coordinate vector minimized during each iteration (Berstekas, 1999). A single iteration takes the form:

$$ x^{k+1}_{i} \in {\mathrm{argmin}_{\varepsilon \in X_i}} = f( x^{k+1}_{1}, ..., x^{k+1}_{i-1}, \varepsilon, x^{k}_{i+1}, ..., x^{k}_{n}) $$ 

For the case of NMF, this scenario is especially simple as there are only two block variables, W and H. Sequentially one matrix is fixed and the other is improved. 


$ W^{k+1} = {\mathrm{argmin}_{W \ge 0} f(W,H^k}) $

$ H^{k+1} = {\mathrm{argmin}_{H \ge 0} f(W^{k+1},H}) $

This takes advantage of the fact that while objective (2.1) is not convex in both W and H, it is convex in either W or H individually. One basic least ALS algorithm to accomplish this outlined by Berry et al (Berry, 2006). This strategy imposes nonnegativity by setting all nonzero entries of W and H to zero during each iteration. Unlike multiplicative update rules, however these zero entries are not forced to remain at 0 for all proceeding iterations. \\

\begin{algorithm}[H]	
 \KwData{Input data matrix:  $\bf{A} \in \Re^{m x n}$}
 \KwResult{nonnegative factorization of \textbf{A} using k components, creating matrices \textbf{W} $ \in \Re^{m x k}$ and \textbf{H} $ \in \Re^{k x n}$ }

 initialization\;
$\bf{W} \gets $ random dense (m x k) matrix\\
$\bf{H} \gets $ random dense (k x n) matrix\\
 \For{i = 1 to \text{maxiter} }{
 	Solve for $\bf{H}$ in $\bf{W^T WH = W^T A} $
	
	set negative elements in $\bf{H}$ to 0
	
 	Solve for $\bf{W}$ in $\bf{HH^T W^T = HA^T}$
	
	set negative elements in $\bf{W}$ to 0	}
 \caption{Alternating least squares}
\end{algorithm} 

This algorithm acts by repeatedly solving unconstrained subproblems and then modifying the W and H solutions each iteration to satisfy the desired non-negativty constraints. By setting all negative values in W and H to zero, the 


\section{Applications to gene expression}
A classical example of NMF is the factorization of a large database of human face images. Factorization using PCA will yield a series of ?eigenfaces? which can be added or subtracted, but individual components are not easy to interpret on their own. The NMF "parts-based" representation however, yields components like noses, eyes, ears, and mouths which can be added linearly to construct a face. Parts-based representations have been helpful in identifying and interpreting patterns in a number of biological contexts. 

NMF has shown to be less sensitive to \textit{a priori} gene selection or initial conditions when identifying context-dependent patterns of gene expression (Brunet, 2004). 

The same properties have been found to be helpful in a number of biological contexts. 

\section{Initialization conditions}

\section{Additional computational issues}
-Sparsity 



\begin{thebibliography}{1}



\bibitem{LeSe} {\sc D. Lee, H. Seung},
{\em Algorithms for Nonnegative Matrix Factorization}, Advances in neural information processing (2001)

\bibitem{Berr} {\sc M. Berry, M. Browne, A. Langville, P. Pauca, R. Plemenos},
{\em Algorithms and applications for approximate nonnegative matrix factorization}, Computational Statistics and Data Analysis 52 (2007) pp. 155 - 173.

\bibitem{Cich} {\sc A. Cichocki, R. Zdunek, A. Huy Phan, S. Amari}, {\em Nonnegative Matrix and Tensor Factorizations}, Wiley, Natick, MA, 2009.

\end{thebibliography}


\end{document}
%% end of file `docultex.tex'
